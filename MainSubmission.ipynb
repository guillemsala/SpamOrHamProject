{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = pd.read_csv(\"fraudulent-e-mails-spam-or-ham/kg_train.csv/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "sub_data_train, sub_data_val, sub_label_train, sub_label_val = train_test_split(train_data, train_data[\"label\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "patterns = [\n",
    "re.compile(r'(?!:\\()[^a-zA-Z0-9 ](?<!:\\()'),\n",
    "re.compile(r'\\s+[a-zA-Z]\\s+'),\n",
    "re.compile(r'\\s+\\d+[a-zA-Z]+|[a-zA-Z]+\\d+\\s+|\\s+[a-zA-Z]+\\d+[a-zA-Z]+\\s+'),\n",
    "re.compile(r'\\^[a-zA-Z]\\s+')]\n",
    "\n",
    "regexHTML = re.compile(r'<[^>]*?>')\n",
    "\n",
    "wordnet_lemma  = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(doc):\n",
    "    list_words = []\n",
    "    for item in doc.split():\n",
    "        list_words.append(wordnet_lemma.lemmatize(item,get_wordnet_pos(item)))\n",
    "    return(' '.join(map(str, list_words)))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # remove all HTML tags\n",
    "    processed_feature = re.sub(regexHTML, ' ', str(text))\n",
    "    \n",
    "    # tags since comments can contain '>' characters\n",
    "    processed_feature = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", ' ', processed_feature)\n",
    "    \n",
    "    # remove numbers attached to strings and single numbers\n",
    "    processed_feature= re.sub(patterns[2], ' ', processed_feature)\n",
    "    \n",
    "    #Remove all the special characters except smileys\n",
    "    processed_feature = re.sub(patterns[0], ' ', processed_feature)\n",
    "    \n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(patterns[1], ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(patterns[3], ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "    \n",
    "    return lemmatize_text(processed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-5826df81c810>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_data_train['processed_text'] = sub_data_train['text'].apply(clean_text)\n",
      "<ipython-input-4-5826df81c810>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_data_val['processed_text'] = sub_data_val['text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "sub_data_train['processed_text'] = sub_data_train['text'].apply(clean_text)\n",
    "sub_data_val['processed_text'] = sub_data_val['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "#TFIDF For the whole corpus\n",
    "count_vectorizer = CountVectorizer(min_df=1) \n",
    "term_freq_matrix = count_vectorizer.fit_transform(sub_data_train['processed_text'])\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(term_freq_matrix)\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf.idf_, index=count_vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf_sorted = df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data_train_ham = sub_data_train.loc[sub_data_train['label']==0, 'processed_text']\n",
    "sub_data_train_spam = sub_data_train.loc[sub_data_train['label']==1, 'processed_text']\n",
    "\n",
    "#TFIDF for HAM\n",
    "count_vectorizer = CountVectorizer(min_df=1) \n",
    "term_freq_matrix_ham = count_vectorizer.fit_transform(sub_data_train_ham)\n",
    "tfidf_ham = TfidfTransformer(norm=\"l2\")\n",
    "tfidf_ham.fit(term_freq_matrix_ham)\n",
    "tf_idf_matrix_ham = tfidf_ham.transform(term_freq_matrix_ham)\n",
    "# print idf values\n",
    "df_idf_ham = pd.DataFrame(tfidf_ham.idf_, index=count_vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf_sorted_ham = df_idf_ham.sort_values(by=['idf_weights'])\n",
    "\n",
    "#TFIDF for SPAM\n",
    "term_freq_matrix_spam = count_vectorizer.fit_transform(sub_data_train_spam)\n",
    "tfidf_spam = TfidfTransformer(norm=\"l2\")\n",
    "tfidf_spam.fit(term_freq_matrix_spam)\n",
    "tf_idf_matrix_spam = tfidf_spam.transform(term_freq_matrix_spam)\n",
    "# print idf values\n",
    "df_idf_spam = pd.DataFrame(tfidf_spam.idf_, index=count_vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf_sorted_spam = df_idf_spam.sort_values(by=['idf_weights'])\n",
    "\n",
    "#Restrict to 30% most common words that appear both in ham TFIDF and spam TFIDF\n",
    "df_idf_sorted_top = df_idf_sorted[:int(df_idf_sorted.size*0.33)].index.tolist()\n",
    "spam_top = df_idf_sorted_spam[:int(df_idf_sorted_spam.size*0.33)].index.tolist()\n",
    "ham_top = df_idf_sorted_ham[:int(df_idf_sorted_ham.size*0.33)].index.tolist()\n",
    "\n",
    "stopwords_top=[]\n",
    "for word in df_idf_sorted_top:\n",
    "    if (word in spam_top) and (word in ham_top):\n",
    "        stopwords_top.append(word)\n",
    "\n",
    "stopwords_top = set(stopwords_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9849162011173185\n"
     ]
    }
   ],
   "source": [
    "#Apart from getting rid of the most common words in the english language, we also get rid of the most common words in both spam and ham\n",
    "bow_transformer = CountVectorizer(strip_accents = 'unicode', stop_words = text.ENGLISH_STOP_WORDS.union(stopwords_top)).fit(sub_data_train['processed_text'])\n",
    "\n",
    "X_train = bow_transformer.transform(sub_data_train['processed_text'])\n",
    "X_val  = bow_transformer.transform(sub_data_val['processed_text'])\n",
    "\n",
    "#Learn Classifier\n",
    "clf = MultinomialNB().fit(X_train, sub_label_train)\n",
    "#Predict Val data\n",
    "pred_val = clf.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(sub_label_val,pred_val)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"fraudulent-e-mails-spam-or-ham/kg_test.csv/kg_test.csv\",encoding='latin-1')\n",
    "X_test = bow_transformer.transform(data_test['text'].apply(clean_text))\n",
    "pred_text = clf.predict(X_test)\n",
    "submission_file = pd.DataFrame({'Id': data_test.index,'Category':pred_text})\n",
    "submission_file.to_csv('to_submit.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

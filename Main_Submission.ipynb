{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = pd.read_csv(\"kg_train.csv/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "sub_data_train, sub_data_val, sub_label_train, sub_label_val = train_test_split(train_data, train_data[\"label\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "!\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "print(string.punctuation)\n",
    "print (re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "patterns = [\n",
    "re.compile(r'[^\\w\\d\\s\\:\\)\\(]'),\n",
    "re.compile(r'\\s+[a-zA-Z]\\s+'),\n",
    "#re.compile(r'\\s+\\d+[a-zA-Z]+|[a-zA-Z]+\\d+\\s+|\\s+[a-zA-Z]+\\d+[a-zA-Z]+\\s+'),\n",
    "re.compile(r'\\s*([0-9])+\\s*'),\n",
    "re.compile(r'\\^[a-zA-Z]\\s+')]\n",
    "\n",
    "regexHTML = re.compile(r'<[^>]*?>')\n",
    "\n",
    "wordnet_lemma  = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(doc):\n",
    "    list_words = []\n",
    "    for item in doc.split():\n",
    "        list_words.append(wordnet_lemma.lemmatize(item,get_wordnet_pos(item)))\n",
    "    return(' '.join(map(str, list_words)))\n",
    "\n",
    "\n",
    "def bag_of_words_not_in_set(words, badwords):\n",
    "    return bag_of_words(set(words) - set(badwords))\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile = 'english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    # remove all HTML tags\n",
    "    processed_feature = re.sub(regexHTML, ' ', str(text))\n",
    "    \n",
    "    # tags since comments can contain '>' characters\n",
    "    processed_feature = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", ' ', processed_feature)\n",
    "    \n",
    "    #Remove all the special characters except :, ) and (\n",
    "    processed_feature = re.sub(patterns[0], ' ', processed_feature)\n",
    "    \n",
    "    # remove numbers attached to strings and single numbers\n",
    "    processed_feature= re.sub(patterns[2], ' ', processed_feature)\n",
    "    \n",
    "    # remove all single characters\n",
    "    #processed_feature= re.sub(patterns[1], ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    #processed_feature = re.sub(patterns[3], ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "    \n",
    "    ######### Correct spelling and grammar: Using Textblob\n",
    "    #blob = TextBlob(processed_feature)\n",
    "    #processed_feature = str(blob.correct())\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "    \n",
    "#     return lemmatize_text(processed_feature)\n",
    "    return processed_feature\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data_train['processed_text'] = sub_data_train['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5650                                                   ok\n",
       "213     the current revcon document includes a paragra...\n",
       "2084                                                  fyi\n",
       "4785                    sorry to confirm they are not in \n",
       "4314    from israel radioby shmuel tal prime minister ...\n",
       "623                                   b release in partb \n",
       "2741    h monday january : am kellyc state gov re: ven...\n",
       "2009    fyi:usibc launches new education initiative to...\n",
       "4930    i will be in and out of the office this week b...\n",
       "3498    report: north korea willing to hold talks with...\n",
       "2151    i ll wait to see him in person next week do we...\n",
       "3539           fyi background for call sheet you will get\n",
       "5364                                       yes i will do \n",
       "3212                                                  fyi\n",
       "823      is latest figure based on end fy data (pakist...\n",
       "2879    jim hoge says that he really would like it by ...\n",
       "5483    ok i m forwarding you more background from sha...\n",
       "4061                                                   ok\n",
       "4252                                     ok safe travels \n",
       "4116                                can you run the traps\n",
       "1314    great you can begin pt one of the best things ...\n",
       "5498          pir preinesmonday august : amhre: just read\n",
       "629     i also haven t been able to get any further de...\n",
       "2959    and which you could dootheriwse it would be ch...\n",
       "2509                                                     \n",
       "5492    clinton is over all it reflects your amazing s...\n",
       "3609                                                   ok\n",
       "5310    it is markedly better but as i m reading thru ...\n",
       "5916    will the dca team be in columbia in early augu...\n",
       "3395                                                  fyi\n",
       "                              ...                        \n",
       "294      : am depart private residence : am en route u...\n",
       "4866                                    very interesting \n",
       "1715    lissa muscatine i would like to call you later...\n",
       "3182                                           non secure\n",
       "4996                                   our posts at work \n",
       "2000                                                 fyi \n",
       "3898    below is mcc s statement and some early articl...\n",
       "974     could you work w derek philippe and others to ...\n",
       "4199    i hope that you can read this ( pages) before ...\n",
       "41      pverveer b friday december : amfrom b pleasele...\n",
       "1278    uup policing meeting with mcguinness breaksdow...\n",
       "2194    you may have been alerted to this incident: th...\n",
       "3709            i still have not received the call sheet \n",
       "3503    will do sullivan jacob j sullivanii state gov√£...\n",
       "3137                                                  fyi\n",
       "205     are you getting any feedback how soon will fox...\n",
       "1068    valmoro lona j friday april : amjiloty lauren ...\n",
       "3152    abedin huma sunday july : pmh (b) (d)b please ...\n",
       "4071    aclbsunday september : pmjake sullivanre: info...\n",
       "2718    h friday november : am jilotylc state gov fw: ...\n",
       "2672    westewelle cannot do call at : holbrooke says ...\n",
       "2459    dear mike congrats on your book you must be ve...\n",
       "1982              i d like to see her can we work it out \n",
       "740                          pls keep the updates coming \n",
       "2121     : am pftesidential daily briefing : am secret...\n",
       "5520    the nbc yorkerdecember the envoyposted by hend...\n",
       "3046                we have italready working on schedule\n",
       "1725    abedin huma friday may : pmcalls call cheryl d...\n",
       "2254    some cats and dogs: talked to denis about eu s...\n",
       "2915    mills cheryl d monday january : pmh doug band ...\n",
       "Name: processed_text, Length: 2382, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_data_train_ham = sub_data_train[sub_data_train['label'] == 0]['processed_text']\n",
    "sub_data_train_spam = sub_data_train[sub_data_train['label'] == 1]['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mill cheryl d monday january 18 2010 2:26 pmh doug band justin cooper cheryl millsmills cheryl dre: glad you re therediane reynolds mmezvinskyjust leave the hospital really rough and sad mark paul and others do the lord plus 10 worki sent keen an email re no security troop just arrive so the hopefully they will be able to keepb doctor there tonight around the clock cdmsent: mon jan 18 14:22:08 : glad you re therepi give me periodic update about what you re see and do love to all'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean_text(train_data['text'][2915])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f443a7075f72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msub_data_train_ham\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#     print ('The doc is \"' + doc + '\"')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mham_tf_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mham_tf_vector_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'd'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mham_tf_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-f443a7075f72>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msub_data_train_ham\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#     print ('The doc is \"' + doc + '\"')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mham_tf_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mham_tf_vector_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'd'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mham_tf_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-f443a7075f72>\u001b[0m in \u001b[0;36mtf\u001b[1;34m(term, document)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-f443a7075f72>\u001b[0m in \u001b[0;36mfreq\u001b[1;34m(term, document)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_lexicon(corpus): # define a set with all possible words included in all the sentences or \"corpus\"\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "\n",
    "\n",
    "vocabulary = build_lexicon(sub_data_train_ham)\n",
    "\n",
    "ham_term_matrix = []\n",
    "# print ('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']\\n')\n",
    "\n",
    "for doc in sub_data_train_ham:    \n",
    "#     print ('The doc is \"' + doc + '\"')\n",
    "    ham_tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    ham_tf_vector_string = ', '.join(format(freq, 'd') for freq in ham_tf_vector)\n",
    "    \n",
    "#     print ('The tf vector for Document %d is [%s]|\\n' % ((sub_data_train_ham.index(doc)+1), ham_tf_vector_string))\n",
    "    ham_term_matrix.append(ham_tf_vector)\n",
    "\n",
    "# print ('\\nAll combined, here is our master document term matrix: ')\n",
    "print (ham_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4174, 52203)\n",
      "(1790, 52203)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5650                                                   ok\n",
       "213     the current revcon document include a paragrap...\n",
       "2084                                                  fyi\n",
       "4785                      sorry to confirm they be not in\n",
       "4314    from israel radioby shmuel tal prime minister ...\n",
       "                              ...                        \n",
       "3046                   we have italready work on schedule\n",
       "1725    abedin huma friday may 22 2009 4:48 pmcalls ca...\n",
       "4079    5 rider haggard close jo borg south africa con...\n",
       "2254    some cat and dogs:1 talk to denis about eu sum...\n",
       "2915    mill cheryl d monday january 18 2010 2:26 pmh ...\n",
       "Name: processed_text, Length: 4174, dtype: object"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# sub_data_train, sub_data_val, sub_label_train, sub_label_val = train_test_split(train_data, train_data[\"label\"], test_size=0.3, random_state=5)\n",
    "\n",
    "bow_transformer = CountVectorizer().fit(sub_data_train['processed_text'])\n",
    "\n",
    "X_train = bow_transformer.transform(sub_data_train['processed_text'])\n",
    "X_val  = bow_transformer.transform(sub_data_val['processed_text'])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "sub_data_train['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9553072625698324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[939,  65],\n",
       "       [ 15, 771]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Learn Classifier\n",
    "clf = MultinomialNB().fit(X_train, sub_label_train)\n",
    "#Predict Val data\n",
    "pred_val = clf.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(sub_label_val,pred_val)\n",
    "print(accuracy)\n",
    "confusion_matrix(sub_label_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"kg_test.csv/kg_test.csv\",encoding='latin-1')\n",
    "X_test = bow_transformer.transform(data_test['text'].apply(clean_text))\n",
    "pred_text = clf.predict(X_test)\n",
    "submission_file = pd.DataFrame({'Id': data_test.index,'Category':pred_text})\n",
    "submission_file.to_csv('to_submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST: Lemmatizing (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SECOND: Spelling and grammar correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIRD: Implement formula to compute efficiency-accuracy ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOURTH: Weird a's show up in dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
